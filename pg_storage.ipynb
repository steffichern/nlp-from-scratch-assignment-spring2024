{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bccd47fc",
      "metadata": {
        "id": "bccd47fc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/postgres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0855d0",
      "metadata": {
        "id": "db0855d0"
      },
      "source": [
        "# Postgres Vector Store\n",
        "In this notebook we are going to show how to use [Postgresql](https://www.postgresql.org) and  [pgvector](https://github.com/pgvector/pgvector)  to perform vector searches in LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f33fc9",
      "metadata": {
        "id": "e4f33fc9"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c2d1c538",
      "metadata": {
        "id": "c2d1c538"
      },
      "outputs": [],
      "source": [
        "# import logging\n",
        "# import sys\n",
        "\n",
        "# Uncomment to see debug logs\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.postgres import PGVectorStore\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c71b6d",
      "metadata": {
        "id": "26c71b6d"
      },
      "source": [
        "### Setup OpenAI\n",
        "The first step is to configure the openai key. It will be used to run inference.\n",
        "\n",
        "Once we switched to local model we don't need this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "67b86621",
      "metadata": {
        "id": "67b86621"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "# import openai\n",
        "\n",
        "# # Reload the variables in your '.env' file (override the existing variables)\n",
        "# dotenv.load_dotenv(\"../.env\", override=True)\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"] "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ab5a1e",
      "metadata": {},
      "source": [
        "Local Embedding Models\n",
        "\n",
        "The easiest way to use a local model is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "560eb92c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
      "metadata": {
        "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396"
      },
      "source": [
        "### Loading documents\n",
        "Load the documents stored in the `data/faculty_websites/` using the SimpleDirectoryReader.\n",
        "\n",
        "Change the documents passed based on your need!\n",
        "\n",
        "You can refer to this to see how to load files in different ways e.g. entire directory\n",
        "\n",
        "https://docs.llamaindex.ai/en/stable/examples/data_connectors/simple_directory_reader.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "c154dd4b",
      "metadata": {
        "id": "c154dd4b",
        "outputId": "e145d64c-7f0e-418f-bf04-c7be01388782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document ID: b04c36ed-87a9-4b45-803c-ea5b95dfb2d9\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_files=[\"./data/faculty_websites/Bhiksharaj_lti_page.txt\"]\n",
        ")\n",
        "docs = reader.load_data()\n",
        "# print(f\"Loaded {len(docs)} docs\")\n",
        "print(\"Document ID:\", docs[0].doc_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd24f0a",
      "metadata": {
        "id": "7bd24f0a"
      },
      "source": [
        "### Create the Database\n",
        "Using an existing postgres running at localhost, create the database we'll be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e6d61e73",
      "metadata": {
        "id": "e6d61e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "password length is 12\n"
          ]
        }
      ],
      "source": [
        "import psycopg2\n",
        "\n",
        "# Reload the variables in your '.env' file (override the existing variables)\n",
        "dotenv.load_dotenv(\"../.env\", override=True)\n",
        "pwd = os.environ['PG_PASSWORD_RAG']\n",
        "user = \"711-rag\"\n",
        "connection_string = f'dbname=postgres user={user} password={pwd}'\n",
        "db_name = \"711-rag\"\n",
        "conn = psycopg2.connect(connection_string)\n",
        "conn.autocommit = True\n",
        "\n",
        "with conn.cursor() as c:\n",
        "    c.execute(f\"DROP DATABASE IF EXISTS \\\"{db_name}\\\"\")\n",
        "    c.execute(f\"CREATE DATABASE \\\"{db_name}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0232fd1",
      "metadata": {
        "id": "c0232fd1"
      },
      "source": [
        "### Create the index\n",
        "Here we create an index backed by Postgres using the documents loaded previously. PGVectorStore takes a few arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "8731da62",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1358441c0c864d87a860820ed8cf2b2c",
            "69a77c77ff1c48cc8107b445ae4fa0cc"
          ]
        },
        "id": "8731da62",
        "outputId": "07d4a9a7-7d10-4483-e2a8-41573db5928d"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import make_url\n",
        "\n",
        "# url = make_url(connection_string)\n",
        "vector_store = PGVectorStore.from_params(\n",
        "    database=db_name,\n",
        "    host=\"localhost\",\n",
        "    password=pwd,\n",
        "    port=5432,\n",
        "    user=user,\n",
        "    table_name=\"all\",\n",
        "    embed_dim=384, \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f10202",
      "metadata": {},
      "source": [
        "The first time you run this code, you will experience error because pgvector extension is not enabled for the database. You can go to terminal and run\n",
        "\n",
        "```\n",
        "sudo -u postgres psql\n",
        "\\c \"711-rag\" # This connects to the 711-rag database we just created\n",
        "CREATE EXTENSION vector;\n",
        "\\q\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "53021416",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50106be1c67642ebb722efd422665f56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb25d0759c9e46349220d86b5fbf3057",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core import Settings\n",
        "# ollama\n",
        "Settings.llm = Ollama(model=\"llama2\", request_timeout=30.0)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    docs, storage_context=storage_context, show_progress=True\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
      "metadata": {
        "id": "8ee4473a-094f-4d0a-a825-e1213db07240"
      },
      "source": [
        "### Query the index\n",
        "We can now ask questions using our index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "0a2bcc07",
      "metadata": {
        "id": "0a2bcc07"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What's the email of Bhiksha Raj?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8cf55bf7",
      "metadata": {
        "id": "8cf55bf7",
        "outputId": "e8826e64-89c6-47cc-ffe1-3774a441a4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The email address of Bhiksha Raj is bhiksha@cs.cmu.edu.\n"
          ]
        }
      ],
      "source": [
        "print(textwrap.fill(str(response), 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bed9e1",
      "metadata": {
        "id": "b3bed9e1"
      },
      "source": [
        "### Querying existing index  (I have not tested the code from here and below!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b2634b",
      "metadata": {
        "id": "e6b2634b"
      },
      "outputs": [],
      "source": [
        "vector_store = PGVectorStore.from_params(\n",
        "    database=\"vector_db\",\n",
        "    host=\"localhost\",\n",
        "    password=\"password\",\n",
        "    port=5432,\n",
        "    user=\"postgres\",\n",
        "    table_name=\"paul_graham_essay\",\n",
        "    embed_dim=1536,  # openai embedding dimension\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7075af3-156e-4bde-8f76-6d9dee86861f",
      "metadata": {
        "id": "e7075af3-156e-4bde-8f76-6d9dee86861f"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did the author do?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b088c090",
      "metadata": {
        "id": "b088c090",
        "outputId": "8cff0c6b-083f-455f-9f77-c625e15b786b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The author worked on writing and programming before college. They wrote short stories and tried\n",
            "writing programs on an IBM 1401 computer. They also built a microcomputer and started programming on\n",
            "it, writing simple games and a word processor. In college, the author initially planned to study\n",
            "philosophy but switched to AI due to their interest in intelligent computers. They taught themselves\n",
            "AI by learning Lisp.\n"
          ]
        }
      ],
      "source": [
        "print(textwrap.fill(str(response), 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55745895-8f01-4275-abaa-b2ebef2cb4c7",
      "metadata": {
        "id": "55745895-8f01-4275-abaa-b2ebef2cb4c7"
      },
      "source": [
        "### Hybrid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91cae40f-3cd4-4403-8af4-aca2705e96a2",
      "metadata": {
        "id": "91cae40f-3cd4-4403-8af4-aca2705e96a2"
      },
      "source": [
        "To enable hybrid search, you need to:\n",
        "1. pass in `hybrid_search=True` when constructing the `PGVectorStore` (and optionally configure `text_search_config` with the desired language)\n",
        "2. pass in `vector_store_query_mode=\"hybrid\"` when constructing the query engine (this config is passed to the retriever under the hood). You can also optionally set the `sparse_top_k` to configure how many results we should obtain from sparse text search (default is using the same value as `similarity_top_k`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a7e133-39da-40c5-b2c5-7af2c0a3a792",
      "metadata": {
        "id": "65a7e133-39da-40c5-b2c5-7af2c0a3a792",
        "outputId": "bd4eed92-b4c2-47e2-8666-0d54a95fa8c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/suo/dev/llama_index/llama_index/vector_stores/postgres.py:217: SAWarning: TypeDecorator TSVector() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this warning at: https://sqlalche.me/e/20/cprf)\n",
            "  session.commit()\n"
          ]
        }
      ],
      "source": [
        "from sqlalchemy import make_url\n",
        "\n",
        "url = make_url(connection_string)\n",
        "hybrid_vector_store = PGVectorStore.from_params(\n",
        "    database=db_name,\n",
        "    host=url.host,\n",
        "    password=url.password,\n",
        "    port=url.port,\n",
        "    user=url.username,\n",
        "    table_name=\"paul_graham_essay_hybrid_search\",\n",
        "    embed_dim=1536,  # openai embedding dimension\n",
        "    hybrid_search=True,\n",
        "    text_search_config=\"english\",\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=hybrid_vector_store\n",
        ")\n",
        "hybrid_index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8edee4-6c19-4d99-b602-110bdc5708e5",
      "metadata": {
        "id": "6f8edee4-6c19-4d99-b602-110bdc5708e5"
      },
      "outputs": [],
      "source": [
        "hybrid_query_engine = hybrid_index.as_query_engine(\n",
        "    vector_store_query_mode=\"hybrid\", sparse_top_k=2\n",
        ")\n",
        "hybrid_response = hybrid_query_engine.query(\n",
        "    \"Who does Paul Graham think of with the word schtick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd454b25-b66c-4733-8ff4-24fb2ee84cec",
      "metadata": {
        "id": "bd454b25-b66c-4733-8ff4-24fb2ee84cec",
        "outputId": "861171c6-2cd8-4d18-9646-498a5b6d5434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Roy Lichtenstein\n"
          ]
        }
      ],
      "source": [
        "print(hybrid_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b274ecb",
      "metadata": {
        "id": "2b274ecb"
      },
      "source": [
        "### PgVector Query Options"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a490a0fa",
      "metadata": {
        "id": "a490a0fa"
      },
      "source": [
        "#### IVFFlat Probes\n",
        "\n",
        "Specify the number of [IVFFlat probes](https://github.com/pgvector/pgvector?tab=readme-ov-file#query-options) (1 by default)\n",
        "\n",
        "When retrieving from the index, you can specify an appropriate number of IVFFlat probes (higher is better for recall, lower is better for speed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111a3682",
      "metadata": {
        "id": "111a3682"
      },
      "outputs": [],
      "source": [
        "retriever = index.as_retriever(\n",
        "    vector_store_query_mode=query_mode,\n",
        "    similarity_top_k=top_k,\n",
        "    vector_store_kwargs={\"ivfflat_probes\": 10},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6104ef8d",
      "metadata": {
        "id": "6104ef8d"
      },
      "source": [
        "#### HNSW EF Search\n",
        "\n",
        "Specify the size of the dynamic [candidate list](https://github.com/pgvector/pgvector?tab=readme-ov-file#query-options-1) for search (40 by default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a44758",
      "metadata": {
        "id": "f3a44758"
      },
      "outputs": [],
      "source": [
        "retriever = index.as_retriever(\n",
        "    vector_store_query_mode=query_mode,\n",
        "    similarity_top_k=top_k,\n",
        "    vector_store_kwargs={\"hnsw_ef_search\": 300},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
