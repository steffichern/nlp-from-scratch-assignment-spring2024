Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Year: 2023
Authors: Fernando Diaz
Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.
Publication Venue: arXiv.org
TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.'}

Full paper text:
Best-Case Retrieval Evaluation: Improving the Sensitivity of
Reciprocal Rank with Lexicographic Precision
Fernando Diaz
Google
MontrÃ©al, QC, Canada
diazf@acm.org
ABSTRACT
Across a variety of ranking tasks, researchers use reciprocal rank
to measure the effectiveness for users interested in exactly one
relevant item. Despite its widespread use, evidence suggests that
reciprocal rank is brittle when discriminating between systems.
This brittleness, in turn, is compounded in modern evaluation set-
tings where current, high-precision systems may be difficult to
distinguish. We address the lack of sensitivity of reciprocal rank by
introducing and connecting it to the concept of best-case retrieval,
an evaluation method focusing on assessing the quality of a ranking
for the most satisfied possible user across possible recall require-
ments. This perspective allows us to generalize reciprocal rank and
define a new preference-based evaluation we call lexicographic pre-
cision or lexiprecision. By mathematical construction, we ensure
that lexiprecision preserves differences detected by reciprocal rank,
while empirically improving sensitivity and robustness across a
broad set of retrieval and recommendation tasks.
1 INTRODUCTION
Evaluating ranking systems for users seeking exactly one relevant
item has a long history in information retrieval. As early as 1968,
Cooper [7]proposed Type 1 expected search length orESL1, defined
as the rank position of the highest ranked relevant item. In the
context of TREC-5, Kantor and Voorhees [12] proposed using the
reciprocal of ESL1in order to emphasize rank changes at the top of
the ranked list and modeling the impatience of a searcher as they
need to scan for a single item. Over the years, reciprocal rank (and
less so ESL1) has established itself as a core metric for retrieval
[5] and recommendation [ 4], adopted in situations where there is
actually only one relevant item as well as in situations where there
are multiple relevant items. Given two rankings, reciprocal rank
andESL1always agree in terms of which ranking is better. Because
of this, we refer to them collectively as the recall level 1 or RL1
metrics.
Despite the widespread use of reciprocal rank, recent evidence
suggests that it may brittle when it comes to discriminating between
ranking systems [ 10,19,20]. In particular, the low number of unique
values of reciprocal rank means that, especially when evaluating
multiple highly-performing systems, we are likely to observe tied
performance. Voorhees et al . [21] demonstrate that these conditions
exist in many modern deep learning benchmarks.
We address these issues by theoretically interpreting RL1as a
population-level metric we refer to as best-case retrieval evaluation .
This allows us to propose a generalization of the RL1ordering based
on social choice theory [ 17] and preference-based evaluation [ 8].
This evaluation method, lexicographic precision or lexiprecision,
(1,2,3,4,5)(1,2,3,4,6)
(n-4,n-3,n-2,n-1,n)(1,2,3,4,7)
(n-5,n-3,n-2,n-1,n)(2,3,4,5,6)(1,n-3,n-2,n-1,n)â€¦â€¦(1,*,*,*,*)(2,*,*,*,*)(3,*,*,*,*)(n-4,n-3,n-2,n-1,n)(n-5,*,*,*,*)â€¦RL1lexicographicprecisionFigure 1: Hasse diagram of possible positions of relevant
items. Each tuple represents the possible positions of five
relevant items in a corpus of ğ‘›items. RL1metrics such as
reciprocal rank (left) have ğ‘›âˆ’4unique values and, therefore,
result in a partial order over all possible positions of relevant
items. Lexicographic precision (right) is a total order over all
possible positions of relevant items that preserves all strict
orders in RL 1evaluation.
preserves any strict ordering between rankings based on RL1while
also providing a theoretically-justified ordering when RL 1is tied.
We compare lexiprecision and RL1orderings using Hasse dia-
grams in Figure 1. On the left, we show the partial order of all
possible positions of five relevant items in a corpus of size ğ‘›. Since
reciprocal rank and ESL1only consider the position of the first
relevant item, we only have ğ‘›different relevance levels. While this
may not be an issue in general (since ğ‘›is usually large), the num-
ber of rankings within each level can be very large and multiple
highly effective systems can result in numerous ties. In contrast,
lexiprecision has one relevance level for each unique arrangement
of relevant items. That is, the number of relevance levels scales
with the number relevant items and, by design, two rankings are
tiedonly if they place relevant items in exactly the same positions.
In this paper, we contribute to the theoretical understanding
of evaluation through a detailed study of RL1metrics, best-case
retrieval evaluation, and lexiprecision. In Section 2, we motivate
our work by showing that RL1has fundamental theoretical limits,
especially in situations where there are multiple relevant items. In
Section 3, we demonstrate that RL1can be interpreted as best-case
retrieval evaluation, allowing us to to address its limitations by
using methods from social choice theory and generalizing it as
lexiprecision. In Section 5, we then conduct extensive empiricalarXiv:2306.07908v1  [cs.IR]  13 Jun 2023
Fernando Diaz
analysis to show that lexiprecision is strongly correlated with RL1
metrics while substantially improving its discriminative power.1
2 MOTIVATION
Our work is based on the observation that ceiling effects are inher-
ent in RL1evaluation. Assume a standard ranking problem where,
given a query with ğ‘šassociated relevant items, a system orders
allğ‘›documents in the collection in decreasing order of predicted
relevance. The set of all possible rankings of ğ‘›is referred to as the
symmetric group over ğ‘›elements and is represented as ğ‘†ğ‘›. For a
given ranking ğœ‹âˆˆğ‘†ğ‘›, letğ‘ğ‘–be the position of the ğ‘–th highest-ranked
relevant item. We can then define reciprocal rank as RR1(ğœ‹)=1
ğ‘1.
When no relevant document is retrieved (e.g. if no relevant items
are in the systemâ€™s top ğ‘˜retrieval), we set RR1=0. For two rank-
ings, we define ğ›¿RR1(ğœ‹, ğœ‹â€²)=RR1(ğœ‹)âˆ’RR1(ğœ‹â€²). For the remainder
of this section, we will use reciprocal rank for clarity although the
analysis applies to ESL 1as well.
Although we can easily see that there are ğ‘›different values for
RR1(ğœ‹), we are interested in the distribution of ties amongst system
rankings for these ğ‘›values as predicted by theoretical properties
of reciprocal rank. Specifically, we want to compute, for a given
position of the first relevant item ğ‘1and a random second ranking,
the probability that we will observe a tie. For any ğ‘1, there are ğ‘›âˆ’ğ‘1
ğ‘šâˆ’1tied arrangements of positions of relevant items amongst all
of the possible arrangements ğ‘â€²from a second system. If we sample
an arrangement of relevant items uniformly at random, then the
probability of a tie with ğœ‹isğ‘ƒğ‘Ÿ(ğ‘1=ğ‘â€²
1|ğ‘1)=(ğ‘›âˆ’ğ‘1
ğ‘šâˆ’1)
(ğ‘›
ğ‘š).
We plot this probability in Figure 2. We can observe that, when
we have few relevant items (i.e. small ğ‘š), we have a relatively small
and uniform probability of ties across all values of ğ‘1. However, as
we increase the number of relevant items, the distribution begins
to skew toward a higher probability of a tie as ğ‘1is smaller. This
means that, if we have a ranking where the first relevant item is
close to the top, even if the second ranking is drawn uniformly at
random, we will be more likely to find a tie than if the first relevant
item were lower in the ranking.
While our analysis indicates a lack of sensitivity of reciprocal
rank for ğ‘â€²drawn uniformly at random as ğ‘šincreases, we are also
interested in the probability of ties when ğ‘â€²is drawn from rank-
ings produced by real systems. We collected runs associated with
multiple public benchmarks (see Section 4.1 for details) and com-
puted the the empirical distribution of ties conditioned ğ‘1(Figure
3). Because of the highly skewed distribution, we plot the logarith-
mic transform of the probability of a rank position. As we can see,
across both older and newer benchmarks, the probability of a tie
for rankings when the top-ranked relevant item is at position 1 is
substantially larger than if we assume ğ‘â€²is drawn uniformly at
random. The 2021 TREC Deep Learning track data in particular
demonstrates higher skew than others, confirming observations
previously made about saturation at top rank positions [21].
Taken together, these results demonstrate a fundamental limita-
tion of RL1metrics (i.e., reciprocal rank and ESL1) for evaluation.
1In lieu of an isolated â€˜Related Workâ€™ section, we have included discussion of relevant
literature when necessary. This helps make connections explicit to our work.
01002003004005000.0000.0010.0020.0030.0040.005
p1Pr(p1=p1')m1050100250Figure 2: Given a ranking where the highest-ranked relevant
item is at position ğ‘1, the probability of a tie with a second
ranking sampled uniformly from all arrangements of rel-
evant items for a corpus size of ğ‘›=50000 . This figure and
others are best rendered or printed in color.
125100.0050.0100.0200.0500.1000.2000.500
p1Pr(p1=p1')robustweb 2013DL 2021 (docs)ml-1m
Figure 3: Empirical probability of a tie with a second ranking
for several benchmarks (see Section 4.1 for details). Horizon-
tal and vertical axes are on a logarithmic scale for clarity.
As retrieval and other scenarios where reciprocal rank is used be-
gin to attract highly performant systems, we need to extend our
evaluation approaches to address these issues.
3 LEXICOGRAPHIC PRECISION
RL1evaluation emphasizes precision by considering the position
of the top-ranked relevant item and ignoring the positions of other
relevant items. However, only ever looking at the position of the top-
ranked relevant item results in the ceiling effects described in the
previous section. Our goal is to develop an evaluation method that
preserves the ordering of a pair of rankings by RL1(i.e., agree with
RR1when RR1(ğœ‹)â‰ RR1(ğœ‹â€²)) and provides a justified ordering
Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
of a pair of rankings when RL1is tied (i.e., generate a sensible
order when RR1(ğœ‹)=RR1(ğœ‹â€²)). Although metrics like expected
reciprocal rank [ 6] and average precision include reciprocal rank
as a component in their computation, they are not guaranteed to
preserve the ordering of reciprocal rank when there is one. In
this section, we will interpret RL1metrics as best-case retrieval
evaluation, allowing us to derive a preference-based evaluation
method based on social choice theory.
3.1 Best-Case Retrieval Evaluation
When a user approaches a retrieval system, there is a great deal of
uncertainty about their information need. While a request such as
a text query provides information about which items in the corpus
might be relevant, it says much less about the userâ€™s appetite for
relevant information. As a result, there is an implicit population of
possible users issuing any particular request, each of whom may
have a different utility for any particular ranking. In this section,
we explore two types of uncertainty and demonstrate that, from
both perspectives, RL1evaluation represents the best-case utility
over that population.
We first consider uncertainty over recall requirements. Robert-
son[15] presented a model for evaluating rankings based on the
diverse set of recall requirements that a user might have. Given a
request and its associated relevant items, users may be interested
in one relevant item, a few relevant items, or the complete set of
all relevant items. We can assess the quality of a ranking for any
particular user recall requirement with what Cooper [7]refers to
as the Type 2 expected search length : the number of items a user
with requirement ğ‘–has to scan before finding ğ‘–relevant items. So,
each information need has ğ‘šrecall levels and RL1is the evaluation
measure associated with users requiring exactly one relevant item.
From this perspective, we can, for a specific ranking, look at how
utility is distributed amongst possible users, as represented by their
recall levels. For example, we can ask how utility for users with high
and low recall requirements compares; or what the average utility
across these populations is. While previous work has looked at the
average-case utility [8] and worst-case utility [9], in this work we
suggest that RL1represents the best-case performance over these
possible users. The proof is relatively simple. Because RRğ‘–monoton-
ically degrades in rank, the best-case utility over this representation
of users is RR1(equivalently ESL1). The next-best-case is RR2and
so forth until we reach RRğ‘š, which we refer to as the worst-case.
So, given two rankings ğœ‹and ğœ‹â€², observing RR1(ğœ‹)>RR1(ğœ‹â€²)
implies that the best-case performance over possible user recall
requirements is higher in ğœ‹compared to ğœ‹â€².
Next, we consider uncertainty over psychologically relevant
items. When evaluating a retrieval system, we often use relevance
labels derived from human assessors or statistical models. But what
if a specific user does not find the top-ranked item labeled relevant
actually relevant to them? For example, a user may have already
seen a specific item or they may desire an item with a specific (miss-
ing) attribute. A judged relevant item might be inappropriate for
any number of reasons not expressed in the request. The concept of
psychological relevance [11] suggests that judging any item relevant
in general (as is the case in many retrieval benchmarks, including
those used in TREC) is a necessary but not sufficient criteria todetermine an itemâ€™s psychological relevance to any particular user.
From this perspective, there are 2ğ‘šâˆ’1possible non-empty sets
of relevant items for a specific request, each representing psycho-
logical relevance to a possible user. Nevertheless, amongst these
possible users, if they are interested in precisely one relevant item,
there are ğ‘šunique utilities. Again, since RL1monotonically de-
creases in rank, the best-case utility is RR1, followed by RR2until
we reach RRğ‘š.
Both uncertainty over recall levels and over psychological rele-
vance focus on possible populations of users. Because the utility
to the user implies utility to the system designer (e.g., for objec-
tives like retention), understanding the best-case performance is
valuable in decision-making. From the perspective of social choice
theory, best-case retrieval evaluation is inherently optimistic and
represents risk-seeking decision-making.
3.2 Lexicographic Precision
The problem with evaluating for best-case retrieval (as shown in
Section 2) is the tendency for multiple rankings to be tied, especially
as (i) we increase the number of relevant items and (ii) systems
optimize for retrieval metrics. We can address these ceiling effects
by developing a best-case preference-based evaluation that focuses
on measuring differences in performance instead of absolute per-
formance [ 8]. While metric-based evaluation models the preference
between rankings by first computing some evaluation metric for
each ranking, preference-based evaluation explicitly models the
preference between two rankings. Prior research has demonstrated
that preference-based evaluation can be much more sensitive than
metric-based evaluation [ 8], making it well-suited for addressing
the ceiling effects described in Section 2.
Under best-case preference-based retrieval, we are interested in
answering the question, â€˜under the best possible scenario, which
ranking would the user prefer?â€™ In this respect, it is a user-based
evaluation method, but one based on preferences and measurement
over a population of users. More formally, given an information
need and two rankings ğœ‹and ğœ‹â€²associated with two systems,
metric-based evaluation uses an evaluation metric ğœ‡:ğ‘†ğ‘›â†’â„œ (e.g.
reciprocal rank or average precision) to compute a preference,
ğœ‡(ğœ‹)>ğœ‡(ğœ‹â€²)=â‡’ğœ‹â‰»ğœ‹â€²
where ğœ‹â‰»ğœ‹â€²indicates that we prefer ğœ‹toğœ‹â€². Notice that, if
ğœ‡(ğœ‹)=ğœ‡(ğœ‹â€²), then we cannot infer a preference between ğœ‹andğœ‹â€².
We contrast this with preference-based evaluation, which directly
models this relationship Î”:ğ‘†ğ‘›Ã—ğ‘†ğ‘›â†’â„œ ,
Î”(ğœ‹, ğœ‹â€²)>0=â‡’ğœ‹â‰»ğœ‹â€²
Our goal is to design a preference-based evaluation that preserves
the best-case properties of RL1metrics with much higher sensitivity.
Consider the two position vectors ğ‘andğ‘â€²in Figure 4 associated
with the two rankings ğœ‹andğœ‹â€².
These two vectors are tied in the best case (i.e., ğ‘1=ğ‘â€²
1). However,
we can break this tie by looking at the next-best case (i.e. ğ‘2) where,
because ğ‘2<ğ‘â€²
2, we say that ğœ‹â‰»ğœ‹â€². If we had observed a tie
between the next-best case, we could compare ğ‘3, and so forth. This
is known as lexicographic sorting in the social choice literature
[17] and reflects a generalization of best-case sorting. Given two
sorted vectors of utilities, here reflected by the rank position, the
Fernando Diaz
2410n-1n289500n
<latexit sha1_base64="0Z4JlA8iYg6awHPgf7vVaWYiDbg=">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> =
<latexit sha1_base64="8Q9MIF5tmntsfpBcQKFy38+B7+U=">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64="fXfk0Lq5jyRW4bqrFK2t5Ax8G70=">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0
(a) Lexicographic PrecisionÎ”(ğœ‹, ğœ‹â€²)
ğ›¿RR1(ğœ‹, ğœ‹â€²) 0
ğ›¿ESL1(ğœ‹, ğœ‹â€²) 0
sgnLP(ğœ‹, ğœ‹â€²) âˆ’ 1
rrLP(ğœ‹, ğœ‹â€²) âˆ’1
8
(b) Preference Magnitude
Figure 4: Lexicographic precision between two rankings ğœ‹
and ğœ‹â€²with ğ‘š=5relevant items in corpus of size ğ‘›. (a)
Using the sorted positions of relevant items, lexiprecision
returns a preference based on the highest-ranked difference
in positions. (b) The magnitude of preference between ğœ‹and
ğœ‹â€²under different schemes.
lexicographic maximum begins by looking at utilities in the best-
off positions (i.e. ğ‘1andğ‘â€²
1) and iteratively inspects lower utility
positions until we find an inequality.
If we exhaust all ğ‘šrelevance levels, we indicate that there is not
preference between the rankings. Note that a tie can only happen if
two rankings have all relevant items in exactly the same positions.
Lexicographic sorting generates a total ordering over all positions
of relevant items, in contrast with just inspecting ğ‘1, which com-
presses all arrangements onto ğ‘›possible values. Because of its basis
in lexicographic ordering, we refer to this lexicographic precision
or lexiprecision.
3.3 Number of Ties Under Lexicographic
Precision
We can contrast the number of ties as ğ‘šincreases in RL1metrics
with the number of ties as ğ‘šincreases in lexiprecision. In the latter,
we only observe ties when the positions of the relevant items for
two rankings are the same and, therefore, we have ğ‘›
ğ‘špossible
â€˜valuesâ€™ and the number of ties given a fixed ranking is constant. If
we add ğ‘˜relevant items, the number of â€˜valuesâ€™ increases , resulting
in an increase in discriminative power. Specifically, if we add ğ‘˜
relevant items to ğ‘š, then the number of possible values scales
exponentially in ğ‘˜.
 ğ‘›
ğ‘š+ğ‘˜
 ğ‘›
ğ‘š=ğ‘š+ğ‘˜Ã–
ğ‘–=ğ‘š+1ğ‘›+1âˆ’ğ‘–
ğ‘–(1)
By contrast, for RL1metrics, this increase in the number of unique
position vectors needs to be allocated to a fixed ğ‘›values, resulting
in collisions, as suggested by the pigeonhole principle. Moreover,
these collisions will tend to increasingly occur at values associated
with position vectors where ğ‘1is small (Section 2).
3.4 Best-Case Retrieval Evaluation Revisited
In Section 3.1, we described two dimensions of uncertainty in re-
trieval evaluation: recall level and psychological relevance. In both
cases, we saw that the best-case utility was represented by RL1.
In terms of preference-based evaluation, we would like to showthat, for both recall level uncertainty and psychological relevance
uncertainty, the highest ranked difference in utility will be ğ›¿RRğ‘–âˆ—,
where ğ‘–âˆ—=argminğ‘—âˆˆ[1,ğ‘š]ğ›¿RRğ‘–(ğœ‹, ğœ‹â€²)â‰ 0. This is clear for recall
level uncertainty because the population of possible users exactly
matches the recall levels defining ğ‘–âˆ—.
However, for psychological relevance uncertainty, we have 2ğ‘šâˆ’1
possible users. That said, there are only ğ‘špossible RL1metric
values. Moreover, the number of possible users tied at the first
recall level is 2ğ‘šâˆ’1; at the second recall level is 2ğ‘šâˆ’2; down to
the final recall level where there is a single possible user. This
arrangement of ties is the same regardless of the exact positions
of the relevant items. Therefore, if we observe ğ›¿RR1=0, we will
observe 2ğ‘šâˆ’1ties amongst the possible psychological relevance
states where where the first relevant item is at position ğ‘1. The next
highest utility is, by the monotonicity of RL1metrics, associated
with the second recall level. We can continue this procedure until
we observe an inequality, which will occur exactly at the first ğ‘–such
that ğ›¿RRğ‘–(ğœ‹, ğœ‹â€²)â‰ 0. In other words, ğ‘–âˆ—.
These observations are important since they demonstrate that
lexiprecision generalizes RL1evaluation and best-case performance
across two types of uncertainty.
3.5 Quantifying Preferences
Although lexiprecision provides a ordering over a pair of rankings,
it does not quantify the magnitude of the preference (i.e. the value
ofÎ”(ğœ‹, ğœ‹â€²)). Defining a magnitude allows us to measure the degree
of preference, which can then be averaged over multiple requests.
We can define the magnitude directly as the value of ğ›¿RRğ‘–and,
therefore, defining Î”(ğœ‹, ğœ‹â€²)as,
rrLP(ğœ‹, ğœ‹â€²)=ğ›¿RRğ‘–âˆ—(ğœ‹, ğœ‹â€²) (2)
where ğ‘–âˆ—is defined in Section 3.4. This has the advantage of, when
ğ‘–âˆ—=1, reproducing the difference in reciprocal rank. Under this
definition, the magnitude of preferences for higher recall levels will
tend to be smaller due to the aggressive discounting in reciprocal
rank.
Alternatively, we can be more conservative in our quantification
and just return a constant value based on the preference, defining
Î”(ğœ‹, ğœ‹â€²)as,
sgnLP(ğœ‹, ğœ‹â€²)=sgn(ğ›¿RRğ‘–âˆ—(ğœ‹, ğœ‹â€²)) (3)
where ğ‘–âˆ—is defined as above. Although the direction of the prefer-
ence agrees with rrLP, we discard its magnitude and, as a result,
differences at lower ranks are equal to those at higher ranks. Prior
work found that looking at unweighted preference information
alone can help with preference sensitivity [8].
3.6 Lexicographic Precision as Modeling ğ›¿RR1
A different way to interpret lexiprecision is as a method to estimate
a high-precision preference between rankings. Assume that we
have some latent preference between two rankings, Ë†Î”(ğœ‹, ğœ‹â€²), that
we know to be â€˜high-precisionâ€™. That is, users prefer finding some
relevant items quickly than allrelevant items quickly.
One way to model this preference is to inspect the positions
of relevant items in ğœ‹andğœ‹â€². From the perspective of â€˜very high
precisionâ€™, observing ğ›¿RR1(ğœ‹, ğœ‹â€²)>0provides significant evidence
that Ë†Î”(ğœ‹, ğœ‹â€²)>0. What if we do not observe a preference at the
Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
51015200.00.20.40.60.81.0
iPr(pi=pi')robustweb 2013DL 2021 (docs)ml-1m
Figure 5: Empirical probability of a tie in position by recall
level. Note that, while Figure 2 measures the probability of
a tie for different positions of the highest ranked relevant
item (i.e. ğ‘1), this figure measures the probability of a tie for
different recall levels.
first recall level? Inspired by Katzâ€™s back-off model [ 13], we inspect
the second recall level for evidence of the value of Ë†Î”(ğœ‹, ğœ‹â€²). If we
do not observe a preference, we can progressively back off to higher
and higher recall levels.
While Section 2 demonstrated that ğ›¿RR1(ğœ‹, ğœ‹â€²)=0with high
probability, backing off our estimates works best if, for ğ‘–>1, we
expect ğ›¿RRğ‘–(ğœ‹, ğœ‹â€²)=0with lower probability. Using the runs
associated with several public benchmarks, we computed ğ›¿RRğ‘–for
all pairs of rankings generated by multiple systems for the same
query. We show the probability of a tie for the first twenty recall
levels in Figure 5. We can see that the number of ties at ğ›¿RR1are
high, ranging from roughly 20
Inspecting the number of relevant items retrieved confirms this.
The DL 2021 submissions had 38.74Â±21.75relevant items in their
retrievals, compared to web with 53.14Â±47.06. Meanwhile, robust
submissions had 40.51Â±41.49relevant items retrieval, suggesting
much higher variance and ml-1m with 7.46Â±8.57relevant items
retrieved and much higher variance, leading to more more ties at
higher recall levels.
Given that different benchmarks observed different behaviors
for ties amongst recall levels, we need to understand how many
recall levels we need to visit before finding evidence for Ë†Î”. If a
benchmark needs many recall levels but observes many ties at high
recall levels, then our model of Ë†Î”may be less reliable. We computed
the number of recall levels needed, ğ‘–âˆ—, for each benchmark and
plotted the empirical cumulative distribution function in Figure 6.
We find that we need fewer than ten recall levels to capture 90
Although our preceding analysis demonstrates that a backoff
model of Ë†Î”based on lexiprecision will terminate at a reasonable
depth, we still need to show that there is locality amongst ğ›¿RRğ‘–.
This means that we ask, if we observe ğ›¿RR1(ğœ‹, ğœ‹â€²)>0, how likely
is it that ğ›¿RR2(ğœ‹, ğœ‹â€²)>0?ğ›¿RR3(ğœ‹, ğœ‹â€²)>0? If there is high locality
amongst ğ›¿RRğ‘–, then information from ğ›¿RRğ‘–+1can help in predicting
051015200.00.20.40.60.81.0
recall levelrobustweb 2013DL 2021 (docs)ml-1mFigure 6: Empirical cumulative distribution function of recall
level needed to distinguish systems.
the true value of ğ›¿RRğ‘–when it is missing or tied. Note that, if we ob-
serve ğ›¿RRğ‘–>0andğ‘›is large, there is absolutely no guarantee that
ğ›¿RRğ‘–+1>0since the next ranked relevant items could, in theory,
occur anywhere in the range [ğ‘ğ‘–+1, ğ‘›]and[ğ‘â€²
ğ‘–+1, ğ‘›]. That said,
given the number of ties at recall level 1, we are interested in under-
standing whether information at other rank positions can provide
a way to distinguish tied rankings. In Figure 7a, we computed the
Pearson correlation amongst all pairs of ğ›¿RRğ‘–forğ‘–âˆˆ[1,8]for the
Robust 2004 benchmark. The fact that correlation between ğ›¿RRğ‘–
andğ›¿RRğ‘–+ğ‘—degrades as ğ‘—increases from 1 demonstrates that there
is indeed high locality. The implication justifies the use of backoff
modeling of Ë†Î”.
To test this hypothesis explicitly, we fit a linear model of ğ›¿RR1
using ğ›¿RR2, . . . , ğ›¿ RR4as independent variables. We plot the coef-
ficients of the linear regression in the solid line in Figure 7b. The
substantially larger coefficient on ğ›¿RR2indicates that the majority
of the predictive power can be found at recall level 2 ( ğ‘—=1). Higher
recall levels ( ğ‘—>1) are associated with much smaller coefficients.
The actual contributions of higher recall levels are much smaller
than this suggests since, because we are operating with reciprocals,
the magnitude of ğ›¿RRğ‘–shrinks as ğ‘–grows. While the colinearity
in Figure 7a might explain some of this disparity in weights, the
locality of individual Pearson correlations and high predictive ac-
curacy means, from a modeling perspective, that a backoff model
is justified. We repeated this analysis for predicting ğ›¿RR2from
ğ›¿RR3, . . . , ğ›¿ RR6and similarly for ğ›¿RR3andğ›¿RR4.
Similar to our observation when modeling ğ›¿RR1, these results
suggest that the next higher recall level is the most valuable predic-
tor when modeling ğ›¿RRğ‘–for any specific recall level.
We repeated this regression analysis for explicitly cascaded data
(i.e. only modeling cases when there is a tie at positions ğ‘–â€²<ğ‘–) as
well as for regressing against the sign of the preference and ob-
served identical findings. Although we omit those plots due to space
constraints, they further support a backoff model intrepretation of
lexiprecision.
Fernando Diaz
Î´RR10.73Î´RR20.620.84Î´RR30.560.740.87Î´RR40.510.670.800.91Î´RR50.470.630.730.840.92Î´RR60.440.590.690.780.860.93Î´RR70.420.550.640.740.800.870.94Î´RR8
(a) Correlation between ğ›¿RRğ‘–
12340.00.20.40.60.81.0predicting Î´RRi using Î´RRi+j
jcoefficienti1 (R2=0.53)2 (R2=0.70)3 (R2=0.76)4 (R2=0.83)
(b) Regression of ğ›¿RRğ‘–using ğ›¿RRğ‘–+1, . . . , ğ›¿ RRğ‘–+4
Figure 7: Locality of ğ›¿RRğ‘–. Relationship between the differ-
ence in reciprocal rank across recall levels using Robust 2004
runs. (a) Pearson and linear fit between all pairs of ğ›¿RRğ‘–. (b)
Linear regression of ğ›¿RRğ‘–using ğ›¿RRğ‘–+1:ğ‘–+4as independent vari-
ables. Regression shown for ğ‘–={1,2,3,4}.
4 METHODS
In previous sections, we theoretically and conceptually connected
RL1to the notion of best-case retrieval evaluation, with a few illus-
trative empirical results. In order to rigorously test the viability of
lexiprecision, we conducted a series of empirical analyses based on
publicly available benchmarking data.2
4.1 Data
We analyzed the performance of lexiprecision across a variety of
retrieval and recommendation tasks. Specifically, we collected runs
submitted to TREC news (Robust 2004, Core 2017 and 2018), web
(Web 2009-2014), and deep learning (Deep Learning 2019-2021)
2Code for computing lexiprecision can be found at https://github.com/diazf/pref_eval.Table 1: Datasets used in empirical analysis.
requests runs rel/request docs/request
news
robust (2004) 249 110 69.93 913.82
core (2017) 50 75 180.04 8853.11
core (2018) 50 72 78.96 7102.61
web
web (2009) 50 48 129.98 925.31
web (2010) 48 32 187.63 7013.21
web (2011) 50 61 167.56 8325.07
web (2012) 50 48 187.36 6719.53
web (2013) 50 61 182.42 7174.38
web (2014) 50 30 212.58 6313.98
deep
deep-docs (2019) 43 38 153.42 623.77
deep-docs (2020) 45 64 39.27 99.55
deep-docs (2021) 57 66 189.63 98.83
deep-pass (2019) 43 37 95.40 892.51
deep-pass (2020) 54 59 66.78 978.01
deep-pass (2021) 53 63 191.96 99.95
recsys
movielens 6005 21 18.87 100.00
libraryThing 7227 21 13.15 100.00
beerAdvocate 17564 21 13.66 99.39
tracks as well as several public recommendation tasks [ 20]. We
present details of these datasets in Table 1.
4.2 Analyses
Our empirical analyses were founded on two core questions, (i) how
empirically correlated are lexiprecision and RL1metrics, and (ii) how
much more robust is lexiprecision than RL1metrics. Because of
its widespread adoption in the research community, we will use
reciprocal rank for analyses. In order to answer the first question,
we conducted experiments designed to predict the agreement be-
tween lexiprecision and RL1metrics under different conditions.
We considered two types of agreement. Agreement in ranking pref-
erence tests whether ğœ‹â‰»LPğœ‹â€²agrees with ğœ‹â‰»RRğœ‹â€². Because
lexiprecision is substantially more sensitive than RL1metrics, we
only consider situations where ğ›¿RR1(ğœ‹, ğœ‹â€²)â‰ 0. Because sgnLP and
rrLP always agree in sign, we will only show results for one of the
metrics when computing ranking agreement. Agreement in system
preference tests whether Eğ‘âˆ¼Qh
Î”LP(ğœ‹ğ‘, ğœ‹â€²ğ‘)i
agrees in sign with
Eğ‘âˆ¼Qh
Î”RR(ğœ‹ğ‘, ğœ‹â€²ğ‘)i
. This measures whether our choice of rrLP or
sgnLP affects its correlation with reciprocal rank. Agreement is
measured as a percentage of preferences agreed upon.
In order to assess the robustness of lexiprecision, we measure
the number of ties observed amongst pairs of rankings and discrim-
inative power. We claim that a robust approach has fewer ties and
higher discriminative power. For discriminative power, we adopt
Sakaiâ€™s approach of measuring the number of statistically signif-
icant differences between runs [ 16], using both Tukeyâ€™s honestly
significant difference (HSD) test [ 3] and classic paired test to com-
pute ğ‘-values. The paired test uses the Studentâ€™s ğ‘¡-test for reciprocal
rank and rrLP [18]; and the binomial test for sgnLP.
Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Table 2: Ranking agreement between ğ›¿RR1and preferences
based on the positions of the last ğ‘šâˆ’1relevant items. The
computation of sgnLP in the table is based on the ğ‘šâˆ’1posi-
tions of relevant items after the top-ranked relevant item.
sgnLP ğ›¿RR2
news
robust (2004) 85.78 83.44
core (2017) 89.23 87.30
core (2018) 88.01 86.58
web
web (2009) 85.87 84.79
web (2010) 87.29 85.41
web (2011) 88.91 87.54
web (2012) 87.22 85.45
web (2013) 86.51 84.45
web (2014) 88.02 85.82
deep
deep-docs (2019) 86.56 83.10
deep-docs (2020) 83.73 79.34
deep-docs (2021) 92.41 89.78
deep-pass (2019) 90.45 88.87
deep-pass (2020) 92.86 91.08
deep-pass (2021) 91.97 90.14
recsys
ml-1M (2018) 78.90 77.56
libraryThing (2018) 66.50 66.08
beerAdvocate (2018) 58.84 58.25
5 RESULTS
5.1 Correlation with Reciprocal Rank
By construction, we know that ğ›¿RR1(ğœ‹, ğœ‹â€²)>0=â‡’LP(ğœ‹, ğœ‹â€²)>
0and, so, the correlation between the two will be high. We can
further test this by comparing how well lexiprecision predicts a
ground truth preference between rankings based on ğ›¿RR1.
In our first analysis, given an observed ğ›¿RR1(ğœ‹, ğœ‹â€²)â‰ 0, we
measure the ability of lexiprecision and reciprocal based only on
ğ‘šâˆ’1subsequent recall levels to predict the sign of ğ›¿RR1(ğœ‹, ğœ‹â€²).
That is, we use ğ›¿RR1(ğœ‹, ğœ‹â€²)as a target value and compute ğ›¿RR1
andLPusing suffixes ğ‘2:ğ‘šandğ‘â€²
2:ğ‘š. Although artificial, this analy-
sis provides an indication of the predictive value gained through
cascaded modeling (as opposed to just looking at the top-ranked
relevant item). We present the results in Table 2. As we can see,
lexiprecision consistently agrees more with the target (masked)
ğ›¿RR1than ğ›¿RR1of the suffix across all datasets, indicating that
the additional information in higher recall levels can be used to
predict the target (masked) ğ›¿RR1. This agrees with our preliminary
analysis in Section 3.6.
We can also test the relationship between reciprocal rank and
lexiprecision by measuring the agreement under incomplete infor-
mation. Specifically, we consider removing either labels (treating
unlabeled items as non-relevant) or requests (i.e. queries or users).
We then measure the agreement between preferences with incom-
plete data and ğ›¿RR1on complete data (i.e. all requests and labels).
Methods that agree more with reciprocal rank on complete data
are considered more correlated. We present results for ranking and
system agreement when removing labels (Figure 8a) and queries
0.70.80.9newsranking
0.60.70.80.9web
0.60.70.80.9deep
0.40.50.60.70.80.9recsys0.20.40.60.8label fractionsign agreement
0.80.9newssystem
0.70.80.9web
0.70.80.9deep
0.90.95recsys0.20.40.60.8label fractionsign agreement(a) Removing labels.
0.70.80.9newssystem
0.70.80.9web
0.50.60.70.80.9deep
0.90.95recsys0.20.40.60.8query fractionsign agreement
(b) Removing queries.
Figure 8: Preference agreement with ğ›¿RR1with full data. La-
bels and requests removed randomly. Results averaged across
ten samples. Solid green lines: ğ›¿RR1with incomplete infor-
mation. Dashed red lines: rrLP with incomplete informa-
tion. Dotted blue lines: sgnLP with incomplete information.
Shaded areas: one standard deviation across samples. Rank-
ing agreement with incomplete labels for sgnLP is identical
to rrLP and omitted for clarity.
(Figure 8b). Across all conditions, we observe that the rrLP has as
Fernando Diaz
Table 3: Percentage of ties between pairs of rankings from
two systems for the same request. We collapse rrLP and
sgnLP for clarity.
rrLP, sgnLP ğ›¿RR1
news
robust (2004) 0.39 44.22
core (2017) 0.23 48.50
core (2018) 1.72 31.43
web
web (2009) 4.93 15.13
web (2010) 0.61 25.85
web (2011) 1.02 41.99
web (2012) 0.34 34.01
web (2013) 0.83 31.09
web (2014) 0.64 41.93
deep
deep-docs (2019) 1.06 68.45
deep-docs (2020) 2.43 73.99
deep-docs (2021) 0.23 80.84
deep-pass (2019) 2.63 56.89
deep-pass (2020) 2.58 50.30
deep-pass (2021) 1.32 47.41
recsys
ml-1M (2018) 3.38 21.39
libraryThing (2018) 16.48 25.85
beerAdvocate (2018) 41.73 45.72
high or slightly higher agreement with ğ›¿RR1with complete infor-
mation than ğ›¿RR1with incomplete information. This means that
rrLP can accurately predict ğ›¿RR1with complete information as well
or better than using reciprocal rank. Moreover, we observed that
sgnLP shows weaker system agreement which occurs because its
magnitude does not decay with rank position and, therefore, result-
ing averages are inconsistent with averages of position-discounted
reciprocal rank values.
5.2 Sensitivity
In Section 2, we motivated our work by showing that RL 1metrics
theoretically and empirically suffer from ceiling effects. The primary
instrument we used to determine this was the probability of ties
between rankings. In Table 3, we present the percentage of tied
rankings from different systems for the same request. As predicted
by our analysis in Section 3.3, lexiprecision has substantially fewer
ties because this only happens when two rankings place relevant
items in exactly the same positions.
In Section 3.3, we showed that lexiprecision implicitly and ex-
ponentially increased its fidelity as the number of relevant items
ğ‘šincreased, while RL1would quickly suffer from ties. In Figure 9,
we show the number of tied rankings as a function of incomplete
labels. This allows us to see trends with respect to ğ‘š. Across our
three retrieval benchmark sets, we see the growth in number of ties
forRL1asğ‘šincreases; meanwhile, they shrink for lexiprecision.
The drop in ties for recommender systems benchmarks suggests
that, as described in Section 3.6, rankings contain very few relevant
items and, as a result, removing labels will result in no relevant
items present and increasingly tied rankings.
00.20.40.6newsranking
00.20.40.6web
00.20.40.6deep
00.20.40.6recsys0.20.40.60.8label fractionfraction tiedFigure 9: Number of ties as labels are removed randomly.
Results are averaged across ten samples. Solid green lines:
ğ›¿RR1with incomplete information. Dashed red lines: rrLP
with incomplete information. Shaded areas: one standard
deviation across samples. Number of ties with incomplete
labels for sgnLP is identical to rrLP and omitted for clarity.
While the number of ties indicates that RL1might not be able to
distinguish systems, for a large enough sample of requests, a met-
ric might still be good enough to distinguish systems. A different
approach to measuring the discriminative power of an evaluation
method is to count the number of differences that are statistically
significant [ 16]. When we compare the percentage of pairs regis-
tering a statistically significant difference (Table 4), both rrLP and
sgnLP outperform reciprocal rank, often by a very large margin.
This indicates that the number of ties indeed hurts the ability of
reciprocal rank to detect significant differences, while both variants
of lexiprecision are much more sensitive.
6 DISCUSSION
Our results demonstrate that our lexiprecision variants capture the
properties of RL1while substantially increasing the ability to dis-
tinguish systems under the same best-case evaluation assumptions.
Practitioners and evaluators need to assess whether the assump-
tions behind RL1metrics, including reciprocal rank, or lexiprecision
or any other evaluation scheme are aligned with the use case. If a
retrieval environment supports the assumptions behind RL1met-
rics, including ties , then, by all means, they should be used to assess
performance. However, in Section 3.1, we raised several reasons
why uncertainty over recall requirements and psychological rel-
evance suggest that RL1metrics make quite strong assumptions
not realized in most retrieval settings. We designed lexiprecision
Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Table 4: Percentage of run differences detected at ğ‘<0.05. Red: better than reciprocal rank. Bold: best for an evaluation setting.
(a) Tukeyâ€™s HSD test
rrLP sgnLP RR
news
robust (2004) 27.42 27.34 23.55
core (2017) 17.41 14.67 15.03
core (2018) 28.60 31.42 27.39
web
web (2009) 23.85 28.28 24.11
web (2010) 18.95 13.51 18.35
web (2011) 14.70 10.22 13.83
web (2012) 13.39 11.61 13.21
web (2013) 5.85 5.79 6.07
web (2014) 20.00 11.72 18.85
deep
deep-docs (2019) 8.25 19.20 6.97
deep-docs (2020) 5.26 3.47 2.88
deep-docs (2021) 6.39 11.19 4.48
deep-pass (2019) 16.52 18.47 13.21
deep-pass (2020) 37.46 40.91 28.35
deep-pass (2021) 24.07 24.78 20.38
recsys
ml-1M (2018) 81.43 90.95 80.00
libraryThing (2018) 93.81 96.67 93.81
beerAdvocate (2018) 92.38 96.19 90.95(b) Paired test with Bonferroni correction
rrLP sgnLP RR
news
robust (2004) 26.22 27.36 21.45
core (2017) 16.22 11.35 11.53
core (2018) 29.30 31.73 27.03
web
web (2009) 23.76 25.18 23.49
web (2010) 18.55 9.27 17.74
web (2011) 12.30 6.94 9.73
web (2012) 11.97 10.11 11.35
web (2013) 4.75 4.64 4.32
web (2014) 15.86 7.13 14.02
deep
deep-docs (2019) 11.66 16.36 5.69
deep-docs (2020) 2.33 1.79 0.60
deep-docs (2021) 3.73 9.14 3.03
deep-pass (2019) 15.02 17.42 10.36
deep-pass (2020) 39.04 39.45 28.00
deep-pass (2021) 23.55 20.99 16.79
recsys
ml-1M (2018) 90.00 92.38 90.48
libraryThing (2018) 97.14 97.62 96.67
beerAdvocate (2018) 94.76 96.67 94.76
to operate as conservatively as possible, preserving any preference
from RL 1metrics and only acting to break ties.
Although RL1metrics and lexiprecision agree perfectly when
there is only one relevant item, this does not mean that all situations
where we have a single judged relevant item should adopt a metric
like reciprocal rank. For example, the MSMARCO dataset [ 14] in-
cludes requests and very sparse labels; the majority of requests have
onejudged relevant item. One might be tempted to use reciprocal
rank but Arabzadeh et al . [1] demonstrate that this would obscure
the multitude of unjudged relevant items (of which there are many).
This hurts efficacy of best-case retrieval evaluation including recip-
rocal rank , as shown in Figures 8a and 9. Recommendation tasks
have similar issues with sparsity due in part to it being more diffi-
cult for a third party to assess the relevance of personalized content
and to the difficulty in gathering explicit feedback. Labels derived
from behavioral feedback in general suffer from similar sparsity [ 2].
In this respect, we echo the call from Arabzadeh et al . [1] to make
labeling practices across all of these domains much more robust.
Given the observation of Voorhees et al . [21] that better labeling
can result in less informative evaluation, we need to also develop
more sensitive evaluation schemes such as lexiprecision.
Finally, this study has introduced a new preference-based eval-
uation method for RL1metrics. As such, our focus has been on
developing an understanding for comparing pairs of rankings and
systems. We do not claim that lexiprecision itself is a metric and
emphasize that we use it for comparing two rankings or systems.
As such, although we address some concerns with reciprocal rank
raised by Ferrante et al . [10] , we do not make claims about lexipreci-
sion being an interval measure. That said, the total ordering shown
in Figure 1 suggests that there may be version of lexiprecision that
can indeed be represented as an interval measure.7 CONCLUSION
Motivated by ceiling effects in evaluation with reciprocal rank, we
have attempted to increase our understanding of the metric and
designed a well-grounded mitigation to conducting best-case re-
trieval evaluation. We have shown that lexiprecision can effectively
address the limitations of reciprocal rank in retrieval evaluation.
Our results highlight the importance of considering the effects of
tie-breaking in the evaluation process and provide a method for
conducting more reliable best-case retrieval evaluation. Given the
use of retrieval metricsâ€”including reciprocal rankâ€”outside of in-
formation retrieval contexts, we believe these contributions will be
relevant to a researchers in the broader research community.
REFERENCES
[1]Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke.
2022. Shallow Pooling for Sparse Labels. Inf. Retr. 25, 4 (dec 2022), 365â€“385.
https://doi.org/10.1007/s10791-022-09411-0
[2]Michael Bendersky, Xuanhui Wang, Marc Najork, and Donald Metzler. 2018.
Learning with Sparse and Biased Feedback for Personal Search. In Proceedings
of the Twenty-Seventh International Joint Conference on Artificial Intelligence,
IJCAI-18 . International Joint Conferences on Artificial Intelligence Organization,
5219â€“5223. https://doi.org/10.24963/ijcai.2018/725
[3] Benjamin A. Carterette. 2012. Multiple testing in statistical analysis of systems-
based information retrieval experiments. ACM Trans. Inf. Syst. 30, 1, Article 4
(March 2012), 34 pages. https://doi.org/10.1145/2094072.2094076
[4]Pablo Castells and Alistair Moffat. 2022. Offline recommender
system evaluation: Challenges and new directions. AI Maga-
zine 43, 2 (2022), 225â€“238. https://doi.org/10.1002/aaai.12051
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12051
[5]Praveen Chandar, Fernando Diaz, and Brian St. Thomas. 2020. Beyond Accu-
racy: Grounding Evaluation Metrics for Human-Machine Learning Systems.
https://github.com/pchandar/beyond-accuracy-tutorial. In Advances in Neural
Information Processing Systems .
[6] Olivier Chapelle, Donald Metzler, Ya Zhang, and Pierre Grinspan. 2009. Expected
reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on
Information and knowledge management (Hong Kong, China) (CIKM â€™09) . ACM,
Fernando Diaz
New York, NY, USA, 621â€“630. https://doi.org/10.1145/1645953.1646033
[7] William S. Cooper. 1968. Expected search length: A single measure of retrieval
effectiveness based on the weak ordering action of retrieval systems. American
Documentation 19, 1 (1968), 30â€“41. https://doi.org/10.1002/asi.5090190108
[8] Fernando Diaz and Andres Ferraro. 2022. Offline Retrieval Evaluation Without
Evaluation Metrics. In Proceedings of the 45th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval .
[9] Fernando Diaz and Bhaskar Mitra. 2023. Recall, Robustness, and Lexicographic
Evaluation. arXiv:2302.11370 [cs.IR]
[10] Marco Ferrante, Nicola Ferro, and Norbert Fuhr. 2021. Towards Meaningful
Statements in IR Evaluation: Mapping Evaluation Measures to Interval Scales.
IEEE Access 9 (2021), 136182â€“136216. https://doi.org/10.1109/ACCESS.2021.
3116857
[11] Stephen P. Harter. 1992. Psychological relevance and information science. Journal
of the American Society for Information Science 43, 9 (1992), 602â€“615.
[12] Paul B Kantor and Ellen Voorhees. 1997. Report on the TREC Confusion Track.
InProceedings of The Fifth Text REtrieval Conference (TREC-5) .
[13] S. Katz. 1987. Estimation of probabilities from sparse data for the language
model component of a speech recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing 35, 3 (1987), 400â€“401. https://doi.org/10.1109/TASSP.1987.
1165125
[14] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary,
Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Gen-
erated MAchine Reading COmprehension Dataset. (November 2016).
https://www.microsoft.com/en-us/research/publication/ms-marco-human-
generated-machine-reading-comprehension-dataset/
[15] Stephen Robertson. 2008. A New Interpretation of Average Precision. In Pro-
ceedings of the 31st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (Singapore, Singapore) (SIGIR â€™08) .Association for Computing Machinery, New York, NY, USA, 689â€“690. https:
//doi.org/10.1145/1390334.1390453
[16] Tetsuya Sakai. 2014. Metrics, statistics, tests. In Bridging Between Information
Retrieval and Databases - PROMISE Winter School 2013, Revised Tutorial Lectures
(Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics)) . Springer Verlag, 116â€“163. https:
//doi.org/10.1007/978-3-642-54798-0_6 2013 PROMISE Winter School: Bridging
Between Information Retrieval and Databases ; Conference date: 04-02-2013
Through 08-02-2013.
[17] Amartya Sen. 1970. Collective Choice and Social Welfare . Holden-Day.
[18] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Sta-
tistical Significance Tests for Information Retrieval Evaluation. In Proceedings of
the Sixteenth ACM Conference on Conference on Information and Knowledge Man-
agement (Lisbon, Portugal) (CIKM â€™07) . Association for Computing Machinery,
New York, NY, USA, 623â€“632. https://doi.org/10.1145/1321440.1321528
[19] Daniel Valcarce, Alejandro BellogÃ­n, Javier Parapar, and Pablo Castells. 2018.
On the Robustness and Discriminative Power of Information Retrieval Met-
rics for Top-N Recommendation. In Proceedings of the 12th ACM Conference
on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys â€™18) .
Association for Computing Machinery, New York, NY, USA, 260â€“268. https:
//doi.org/10.1145/3240323.3240347
[20] Daniel Valcarce, Alejandro BellogÃ­n, Javier Parapar, and Pablo Castells. 2020.
Assessing ranking metrics in top-N recommendation. Information Retrieval
Journal 23, 4 (2020), 411â€“448. https://doi.org/10.1007/s10791-020-09377-x
[21] Ellen M. Voorhees, Nick Craswell, and Jimmy Lin. 2022. Too Many Relevants:
Whither Cranfield Test Collections?. In Proceedings of the 45th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Madrid,
Spain) (SIGIR â€™22) . Association for Computing Machinery, New York, NY, USA,
2970â€“2980. https://doi.org/10.1145/3477495.3531728
